{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e7276-1d18-4c80-a2d6-a3ab77321f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39567df-0953-43a7-9141-f199894f7b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e863397-0835-4a30-8cd5-8b7831eb8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# Load policy model (Qwen2.5-0.5B-Instruct)\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"What is the capital of France?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645f8d9-a9d2-4e61-8cfe-a37ac3e7a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "\n",
    "# Trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1cff9-99cf-47da-b634-a49442264cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # rank of LoRA (smaller = lighter)\n",
    "    lora_alpha=16,             # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # depends on model arch\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "policy_model = get_peft_model(policy_model, lora_config)\n",
    "\n",
    "# Optional: See how many parameters will be trained\n",
    "policy_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1dd38-1f0b-4cc4-8679-5e1f27a1949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # or load_in_8bit=True if you prefer\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"OpenAssistant/reward-model-deberta-v3-large\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/reward-model-deberta-v3-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3b606-e2d8-4336-a5a7-08068a5b47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load Prompts\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "prompts = [sample[\"instruction\"] for sample in dataset.select(range(1000))]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03597db-d62c-460a-b6bf-311f48d40de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Step 0: Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "policy_model.train()\n",
    "\n",
    "# Optimizer only for LoRA trainable parameters\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, policy_model.parameters()), lr=1e-4)\n",
    "\n",
    "# Baseline for reward\n",
    "baseline_reward = 0.0\n",
    "gamma = 0.99  # discount\n",
    "\n",
    "clip_eps = 0.2  # PPO clip range\n",
    "batch_size = 4  # Set your batch size\n",
    "\n",
    "policy_tokenizer.padding_side = \"left\"\n",
    "reward_tokenizer.padding_side = \"left\"\n",
    "policy_tokenizer.padding_side = \"left\"\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Make batches of prompts\n",
    "for batch_idx in range(0, len(prompts), batch_size):\n",
    "    \n",
    "    batch_prompts = prompts[batch_idx:batch_idx+batch_size]\n",
    "\n",
    "    # 1. Tokenize batch of prompts\n",
    "    inputs = policy_tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    # 2. Generate batch of responses\n",
    "    full_output = policy_model.generate(**inputs, max_new_tokens=128, do_sample=True, pad_token_id=policy_tokenizer.pad_token_id)\n",
    "\n",
    "    # 3. Get prompt lengths to split\n",
    "    prompt_lengths = inputs.input_ids.shape[1]  # Assume fixed prompt length in batch\n",
    "\n",
    "    generated_tokens = full_output[:, prompt_lengths:]\n",
    "\n",
    "    # 4. Decode responses\n",
    "    responses_text = policy_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # 5. Compute rewards for each response\n",
    "    reward_inputs = reward_tokenizer(responses_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    reward_logits = reward_model(**reward_inputs).logits\n",
    "    reward_scores = reward_logits.squeeze()  # shape: (batch_size,)\n",
    "\n",
    "    # 6. Compute advantages\n",
    "    advantages = reward_scores - baseline_reward  # vectorized\n",
    "\n",
    "    # 7. Compute old logprobs\n",
    "    with torch.no_grad():\n",
    "        full_logits = policy_model(full_output).logits\n",
    "\n",
    "    log_probs = F.log_softmax(full_logits[:, prompt_lengths-1:-1, :], dim=-1)\n",
    "    chosen_log_probs = torch.gather(log_probs, 2, generated_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "    old_log_probs = chosen_log_probs.detach()\n",
    "\n",
    "    # 8. Forward current policy again\n",
    "    current_logits = policy_model(full_output).logits\n",
    "    current_log_probs = F.log_softmax(current_logits[:, prompt_lengths-1:-1, :], dim=-1)\n",
    "    current_chosen_log_probs = torch.gather(current_log_probs, 2, generated_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # 9. Calculate ratio\n",
    "    log_ratio = (current_chosen_log_probs - old_log_probs)\n",
    "    ratio = torch.exp(log_ratio)\n",
    "\n",
    "    # 10. PPO clipped loss\n",
    "    unclipped_loss = -advantages.unsqueeze(1) * ratio\n",
    "    clipped_loss = -advantages.unsqueeze(1) * torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps)\n",
    "    policy_loss = torch.max(unclipped_loss, clipped_loss).mean()\n",
    "\n",
    "    # 11. Optimize\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 12. Update baseline reward\n",
    "    baseline_reward = gamma * baseline_reward + (1 - gamma) * reward_scores.mean().item()\n",
    "\n",
    "    print(f\"Batch {batch_idx//batch_size}: Avg Reward={reward_scores.mean():.5f}, Avg Advantage={advantages.mean():.5f}, Loss={policy_loss.item():.5f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305193b-b7d1-4c96-85e5-0a040790489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is the capital of France?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output = policy_model.generate(input_ids)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94028bd-4215-441d-b09e-abc8355a3f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d264d-db82-449f-a8e4-7326155c17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca365fd-9629-4e82-ad98-a637f6d917d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "policy_model.train()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, policy_model.parameters()), lr=1e-4)\n",
    "\n",
    "policy_tokenizer.padding_side = \"left\"\n",
    "reward_tokenizer.padding_side = \"left\"\n",
    "policy_tokenizer.padding_side = \"left\"\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "# PPO Hyperparameters\n",
    "batch_size = 4\n",
    "clip_eps = 0.2\n",
    "gamma = 0.99  # baseline discount\n",
    "max_prompt_len = 512\n",
    "max_gen_tokens = 128\n",
    "\n",
    "# KL regularization parameters\n",
    "beta_kl = 0.01        # initial KL penalty weight\n",
    "target_kl = 0.02      # target KL divergence\n",
    "kl_adapt = 1.5        # adaptation factor for beta\n",
    "\n",
    "# Initialize reward baseline\n",
    "baseline_reward = 0.0\n",
    "\n",
    "# Setup tqdm progress bar\n",
    "pbar = tqdm(range(0, len(prompts), batch_size), desc=\"PPO Training\", dynamic_ncols=True)\n",
    "\n",
    "for batch_idx in pbar:\n",
    "\n",
    "    batch_prompts = prompts[batch_idx:batch_idx+batch_size]\n",
    "\n",
    "    # 1. Tokenize prompts\n",
    "    inputs = policy_tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_prompt_len).to(device)\n",
    "\n",
    "    # 2. Generate responses\n",
    "    full_output = policy_model.generate(**inputs, max_new_tokens=max_gen_tokens, do_sample=True, pad_token_id=policy_tokenizer.pad_token_id)\n",
    "\n",
    "    # 3. Get prompt lengths\n",
    "    prompt_len = inputs.input_ids.shape[1]\n",
    "    generated_tokens = full_output[:, prompt_len:]\n",
    "\n",
    "    # 4. Decode responses\n",
    "    responses_text = policy_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # 5. Compute rewards\n",
    "    reward_inputs = reward_tokenizer(responses_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_prompt_len).to(device)\n",
    "    reward_logits = reward_model(**reward_inputs).logits\n",
    "    reward_scores = reward_logits.squeeze()  # (batch_size,)\n",
    "\n",
    "    # 6. Compute advantages\n",
    "    advantages = reward_scores - baseline_reward  # (batch_size,)\n",
    "\n",
    "    # 7. Compute old logprobs\n",
    "    with torch.no_grad():\n",
    "        full_logits_old = policy_model(full_output).logits\n",
    "\n",
    "    log_probs_old = F.log_softmax(full_logits_old[:, prompt_len-1:-1, :], dim=-1)\n",
    "    chosen_log_probs_old = torch.gather(log_probs_old, 2, generated_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "    old_log_probs = chosen_log_probs_old.detach()\n",
    "\n",
    "    # 8. Forward pass current model\n",
    "    full_logits_current = policy_model(full_output).logits\n",
    "    log_probs_current = F.log_softmax(full_logits_current[:, prompt_len-1:-1, :], dim=-1)\n",
    "    chosen_log_probs_current = torch.gather(log_probs_current, 2, generated_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # 9. Calculate ratio\n",
    "    log_ratio = chosen_log_probs_current - old_log_probs\n",
    "    ratio = torch.exp(log_ratio)\n",
    "\n",
    "    # 10. Calculate KL divergence (forward KL)\n",
    "    kl_per_token = old_log_probs.exp() * (old_log_probs - chosen_log_probs_current)\n",
    "    kl_batch = kl_per_token.sum(dim=1).mean()  # average over batch\n",
    "\n",
    "    # 11. PPO surrogate loss\n",
    "    unclipped_loss = -advantages.unsqueeze(1) * ratio\n",
    "    clipped_loss = -advantages.unsqueeze(1) * torch.clamp(ratio, 1-clip_eps, 1+clip_eps)\n",
    "    ppo_loss = torch.max(unclipped_loss, clipped_loss).mean()\n",
    "\n",
    "    # 12. Total loss = PPO loss + KL penalty\n",
    "    total_loss = ppo_loss + beta_kl * kl_batch\n",
    "\n",
    "    # 13. Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 14. Update baseline reward\n",
    "    baseline_reward = gamma * baseline_reward + (1 - gamma) * reward_scores.mean().item()\n",
    "\n",
    "    # 15. Adapt beta_kl if needed\n",
    "    if kl_batch > target_kl * 1.5:\n",
    "        beta_kl *= kl_adapt\n",
    "    elif kl_batch < target_kl / 1.5:\n",
    "        beta_kl /= kl_adapt\n",
    "\n",
    "        # Update tqdm live bar\n",
    "    pbar.set_postfix({\n",
    "        \"loss\": f\"{total_loss.item():.5f}\",\n",
    "        \"reward\": f\"{reward_scores.mean().item():.5f}\",\n",
    "        \"adv\": f\"{advantages.mean().item():.5f}\",\n",
    "        \"kl\": f\"{kl_batch.item():.5f}\",\n",
    "        \"beta_kl\": f\"{beta_kl:.5f}\"\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee27e3-e938-482a-8798-5bc93c241072",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is the animal: dog or house?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output = policy_model.generate(input_ids)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e5d88-468a-4048-b905-8c18ffdd9c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
