{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428b5a2-e31c-4bce-bfe9-4e83ee8d6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Path to your checkpoint folder (change if needed)\n",
    "MODEL_DIR = \"./models/minimal/ppo_tldr/checkpoint-1\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Use text generation pipeline (optional, for convenience)\n",
    "summarizer = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d706d9-a9fc-488f-8aca-802a03e2197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nino/miniconda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 3 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:46<00:00, 35.34s/it]\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.02s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"vwxyzjn/ppo_tldr\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"vwxyzjn/ppo_tldr\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vwxyzjn/ppo_tldr_6.9b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"vwxyzjn/ppo_tldr_6.9b\")\n",
    "\n",
    "summarizer = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19190fa2-78d2-4ff0-8b0c-1cc6a2eb0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: TL;DR:  Proximal policy optimization (PPO) is a reinforcement learning (RL) algorithm for training an intelligent agent. Specifically, it is a policy gradient method, often used for deep RL when the policy network is very large.\n",
      "History\n",
      "\n",
      "The predecessor to PPO, Trust Region Policy Optimization (TRPO), was published in 2015. It addressed the instability issue of another algorithm, the Deep Q-Network (DQN), by using the trust region method to limit the KL divergence between the old and new policies. However, TRPO uses the Hessian matrix (a matrix of second derivatives) to enforce the trust region, but the Hessian is inefficient for large-scale problems.[1]\n",
      "\n",
      "PPO was published in 2017. It was essentially an approximation of TRPO that does not require computing the Hessian. The KL divergence constraint was approximated by simply clipping the policy gradient.[2]\n",
      "\n",
      "Since 2018, PPO was the default RL algorithm at OpenAI.[3] PPO has been applied to many areas, such as controlling a robotic arm, beating professional players at Dota 2 (OpenAI Five), and playing Atari games.[4] \n",
      "To begin the PPO training process, the agent is set in an environment to perform actions based on its current input. In the early phase of training, \n",
      "the agent can freely explore solutions and keep track of the result. Later, with a certain amount of transition samples and policy updates, \n",
      "the agent will select an action to take by randomly sampling from the probability distribution P ( A | S ) {\\displaystyle P(A|S)} \n",
      "generated by the policy network.[8] The actions that are most likely to be beneficial will have the highest probability of being selected \n",
      "from the random sample. After an agent arrives at a different scenario (a new state) by acting, it is rewarded with a positive reward or a negative reward.\n",
      "The objective of an agent is to maximize the cumulative reward signal across sequences of states, known as episodes. #### \n",
      "\n",
      "The learning process can be divided into three main steps: the experience replay step, the policy update step, and the reward evaluation step. A new experience sample is created when the agent reaches a new state, and it is used to update the policy and learn the reward function. During training, the\n"
     ]
    }
   ],
   "source": [
    "# Example long text to summarize\n",
    "input_text = (\n",
    "   \"\"\" Proximal policy optimization (PPO) is a reinforcement learning (RL) algorithm for training an intelligent agent. Specifically, it is a policy gradient method, often used for deep RL when the policy network is very large.\n",
    "History\n",
    "\n",
    "The predecessor to PPO, Trust Region Policy Optimization (TRPO), was published in 2015. It addressed the instability issue of another algorithm, the Deep Q-Network (DQN), by using the trust region method to limit the KL divergence between the old and new policies. However, TRPO uses the Hessian matrix (a matrix of second derivatives) to enforce the trust region, but the Hessian is inefficient for large-scale problems.[1]\n",
    "\n",
    "PPO was published in 2017. It was essentially an approximation of TRPO that does not require computing the Hessian. The KL divergence constraint was approximated by simply clipping the policy gradient.[2]\n",
    "\n",
    "Since 2018, PPO was the default RL algorithm at OpenAI.[3] PPO has been applied to many areas, such as controlling a robotic arm, beating professional players at Dota 2 (OpenAI Five), and playing Atari games.[4] \n",
    "To begin the PPO training process, the agent is set in an environment to perform actions based on its current input. In the early phase of training, \n",
    "the agent can freely explore solutions and keep track of the result. Later, with a certain amount of transition samples and policy updates, \n",
    "the agent will select an action to take by randomly sampling from the probability distribution P ( A | S ) {\\displaystyle P(A|S)} \n",
    "generated by the policy network.[8] The actions that are most likely to be beneficial will have the highest probability of being selected \n",
    "from the random sample. After an agent arrives at a different scenario (a new state) by acting, it is rewarded with a positive reward or a negative reward.\n",
    "The objective of an agent is to maximize the cumulative reward signal across sequences of states, known as episodes. #### \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Format as a TL;DR prompt (matches how it was trained)\n",
    "prompt = f\"TL;DR: {input_text}\"\n",
    "\n",
    "# Generate TL;DR-style summary\n",
    "outputs = summarizer(prompt, max_new_tokens=60, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "# Print result\n",
    "print(\"Generated Summary:\", outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39967c05-1346-4aa2-8f54-a88ba9306cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
